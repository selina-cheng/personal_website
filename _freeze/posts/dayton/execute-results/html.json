{
  "hash": "dd026e50b6d4421cb570288f00891a86",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Is WGI World Championships Predictable?\nformat:\n  html:\n    code-fold: true\nexecute:\n  freeze: true\n---\n\n# Web Scraping\n\nUse Selenium as a robot to click on all the necessary links and save dynamic HTML for each event in World Championships\n\n::: {#ec43d73d .cell execution_count=1}\n``` {.python .cell-code}\n# Import necessary dependencies\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n# Using Firefox webdriver\ndriver = webdriver.Firefox()\ndriver.get(\"https://wgi.org/scores/\") \ndriver.fullscreen_window()\n\n# Remove Cookies pop-up at bottom\nWebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, \"cookie_action_close_header\"))).click()\n\n# Click on \"View Past Seasons\" to access all years\nWebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.LINK_TEXT, \"View Past Seasons\"))).click()\n\n# List to store HTML source of each desired event\npages = []\n\n# Click on each year\nyears = driver.find_elements(By.CLASS_NAME, \"event\")\n\nfor n, yr in enumerate(years):\n    years = driver.find_elements(By.CLASS_NAME, \"event\")\n    \n    # Scroll to year\n    driver.execute_script(\"arguments[0].scrollIntoView(true);\", years[n])\n\n    # Click year when visible\n    WebDriverWait(driver, 30).until(EC.element_to_be_clickable(years[n])).click()\n\n    # All Championship Events\n    world_events = driver.find_elements(By.XPATH, \"//div[contains(text(), 'Championships')]\")\n\n    for m, w in enumerate(world_events):\n        world_events = driver.find_elements(By.XPATH, \"//div[contains(text(), 'Championships')]\")\n\n        if world_events:\n            # Scroll to event and click when visible\n            try:\n                driver.execute_script(\"arguments[0].scrollIntoView(true);\", world_events[m])\n                WebDriverWait(driver, 30).until(EC.element_to_be_clickable(world_events[m])).click()\n\n                # Add HTML sources to list\n                pages.append(driver.find_element(By.XPATH, \"//*\").get_attribute('innerHTML'))\n\n            except Exception as e:\n                print(\"Error: \", e)\n                continue\n            \n            # Scroll to top\n            top = WebDriverWait(driver, 30).until(\n            EC.presence_of_element_located((By.LINK_TEXT, \"View Event List\")))\n\n            driver.execute_script(\"arguments[0].scrollIntoView(false);\", top)\n\n            # Go back \n            WebDriverWait(driver, 30).until(EC.element_to_be_clickable(top)).click()\n\n    world_events = driver.find_elements(By.XPATH, \"//div[contains(text(), 'IW')]\")\n\n    # All PIW Events\n    for m, w in enumerate(world_events):\n        world_events = driver.find_elements(By.XPATH, \"//div[contains(text(), 'IW')]\")\n\n        # Scroll to event and click when visible\n        try:\n            driver.execute_script(\"arguments[0].scrollIntoView(true);\", world_events[m])\n            WebDriverWait(driver, 30).until(EC.element_to_be_clickable(world_events[m])).click()\n\n            # Add HTML sources to list\n            pages.append(driver.find_element(By.XPATH, \"//*\").get_attribute('innerHTML'))\n\n        except Exception as e:\n            print(\"Error: \", e)\n            continue\n        \n        # Scroll to top\n        top = driver.find_element(By.LINK_TEXT, \"View Event List\")\n        driver.execute_script(\"arguments[0].scrollIntoView(false);\", top)\n\n        # Go back \n        WebDriverWait(driver, 30).until(EC.element_to_be_clickable(top)).click()\n\n    # Go back to list of years\n    WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.LINK_TEXT, \"View Past Seasons\"))).click()\n```\n:::\n\n\nUse pickle to save list to prevent having to run Selenium again.\n\n::: {#8cb7075b .cell execution_count=2}\n``` {.python .cell-code}\nimport pickle\n\nwith open('dayton_html.pkl', 'wb') as f:\n    pickle.dump(pages, f, pickle.HIGHEST_PROTOCOL)\n```\n:::\n\n\n::: {#60018941 .cell execution_count=3}\n``` {.python .cell-code}\n# Reload list of HTML data if necessary\nimport pickle\n\nwith open('dayton_html.pkl', 'rb') as f:\n    pages = pickle.load(f)\n```\n:::\n\n\nUse BeautifulSoup to parse HTML data and compile it into a data set.\n\n::: {#43f285fb .cell execution_count=4}\n``` {.python .cell-code}\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# List of dataframes storing results from each page\ndfs = []\n\nfor page in pages:\n    doc = BeautifulSoup(page, \"html.parser\")\n    event_name = doc.find_all(id=\"cs-org-scores-header\")\n    rank_col = doc.find_all(\"div\", \"rank\")\n    group_col = doc.find_all(\"div\", \"group\")\n    score_col = doc.find_all(\"div\", \"score\")\n\n    # Create dataframe of this page of data\n    data = {\"Event\": [event_name] * len(rank_col),\n            \"Rank\": rank_col,\n            \"Group\": group_col,\n            \"Score\": score_col}\n    event = pd.DataFrame(data)\n    dfs.append(event)\n\n# Final dataset\nworld_df = pd.concat(dfs, ignore_index = True)\n```\n:::\n\n\n# Data Analysis\n\nGet basic info of the dataset.\n\n::: {#6d9df279 .cell execution_count=5}\n``` {.python .cell-code}\n# First 5 rows of data\nworld_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Event</th>\n      <th>Rank</th>\n      <th>Group</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[[2023 WGI World Championships Winds Finals]]</td>\n      <td>[1]</td>\n      <td>[Bob Jones HS]</td>\n      <td>[93.850]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[[2023 WGI World Championships Winds Finals]]</td>\n      <td>[2]</td>\n      <td>[Valley Christian HS]</td>\n      <td>[91.650]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[[2023 WGI World Championships Winds Finals]]</td>\n      <td>[3]</td>\n      <td>[Bellevue West HS]</td>\n      <td>[90.700]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[[2023 WGI World Championships Winds Finals]]</td>\n      <td>[4]</td>\n      <td>[Papillion-La Vista South HS]</td>\n      <td>[86.600]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[[2023 WGI World Championships Winds Finals]]</td>\n      <td>[5]</td>\n      <td>[Father Ryan HS]</td>\n      <td>[85.150]</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#731a6edc .cell execution_count=6}\n``` {.python .cell-code}\n# General info\nprint(world_df.info())\n\nworld_df.describe()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30440 entries, 0 to 30439\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Event   30440 non-null  object\n 1   Rank    30440 non-null  object\n 2   Group   30440 non-null  object\n 3   Score   30440 non-null  object\ndtypes: object(4)\nmemory usage: 951.4+ KB\nNone\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Event</th>\n      <th>Rank</th>\n      <th>Group</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>30440</td>\n      <td>30440</td>\n      <td>30440</td>\n      <td>30440</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>214</td>\n      <td>25</td>\n      <td>1255</td>\n      <td>2748</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>[[2023 WGI World Championships IO RD 1 &amp; 3, SW...</td>\n      <td>[1]</td>\n      <td>[Avon HS]</td>\n      <td>[83.300]</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1036</td>\n      <td>3197</td>\n      <td>170</td>\n      <td>104</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Questions\n\n\n\n\n# Reflection\n\n## Technical Learning\n\nlearning python syntax\n\n`enumerate()`\n\ndebugging lots of errors\n\nlearning DOM and HTML \n\n**wgi is a dynamic website**\n\nselenium quirks (can't click on an element if it's not visible on the page)\n\n- basically dont do ANYTHING (finding an element, clikcing elemnt, scorlling to element) until sleenium recognize the leement is loaded\n\n`StaleElementError`\n\ndifferent names for World Class finals through all the years.. \n    using IW and Championships will create some duplicates.. but its possible to remove duplicate data :D\n\ntook like 15 mins to scrape the data LOL not even including 2020-2021 cuz those years did not exist\n\n",
    "supporting": [
      "dayton_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}