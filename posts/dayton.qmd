---
title: "Is WGI World Championships Predictable?"
description: "Web Scraping and Exploratory Data Analysis with Selenium, BeautifulSoup, Pandas"
date: "3/21/24"
format:
  html:
    code-fold: false
jupyter: python3
execute:
    freeze: true
draft: true
---


# Web Scraping

Use Selenium as a robot to click on all the necessary links and save dynamic HTML for each event in World Championships

```{python}
#| eval: false

# Import necessary dependencies
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Using Firefox webdriver
driver = webdriver.Firefox()
driver.get("https://wgi.org/scores/") 
driver.fullscreen_window()

# Remove Cookies pop-up at bottom
WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, "cookie_action_close_header"))).click()

# Click on "View Past Seasons" to access all years
WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.LINK_TEXT, "View Past Seasons"))).click()

# List to store HTML source of each desired event
pages = []

# Click on each year
years = driver.find_elements(By.CLASS_NAME, "event")

for n, yr in enumerate(years):
    years = driver.find_elements(By.CLASS_NAME, "event")
    
    # Scroll to year
    driver.execute_script("arguments[0].scrollIntoView(true);", years[n])

    # Click year when visible
    WebDriverWait(driver, 30).until(EC.element_to_be_clickable(years[n])).click()

    # All Championship Events
    world_events = driver.find_elements(By.XPATH, "//div[contains(text(), 'Championships')]")

    for m, w in enumerate(world_events):
        world_events = driver.find_elements(By.XPATH, "//div[contains(text(), 'Championships')]")

        if world_events:
            # Scroll to event and click when visible
            try:
                driver.execute_script("arguments[0].scrollIntoView(true);", world_events[m])
                WebDriverWait(driver, 30).until(EC.element_to_be_clickable(world_events[m])).click()

                # Add HTML sources to list
                pages.append(driver.find_element(By.XPATH, "//*").get_attribute('innerHTML'))

            except Exception as e:
                print("Error: ", e)
                continue
            
            # Scroll to top
            top = WebDriverWait(driver, 30).until(
            EC.presence_of_element_located((By.LINK_TEXT, "View Event List")))

            driver.execute_script("arguments[0].scrollIntoView(false);", top)

            # Go back 
            WebDriverWait(driver, 30).until(EC.element_to_be_clickable(top)).click()

    world_events = driver.find_elements(By.XPATH, "//div[contains(text(), 'IW')]")

    # All PIW Events
    for m, w in enumerate(world_events):
        world_events = driver.find_elements(By.XPATH, "//div[contains(text(), 'IW')]")

        # Scroll to event and click when visible
        try:
            driver.execute_script("arguments[0].scrollIntoView(true);", world_events[m])
            WebDriverWait(driver, 30).until(EC.element_to_be_clickable(world_events[m])).click()

            # Add HTML sources to list
            pages.append(driver.find_element(By.XPATH, "//*").get_attribute('innerHTML'))

        except Exception as e:
            print("Error: ", e)
            continue
        
        # Scroll to top
        top = driver.find_element(By.LINK_TEXT, "View Event List")
        driver.execute_script("arguments[0].scrollIntoView(false);", top)

        # Go back 
        WebDriverWait(driver, 30).until(EC.element_to_be_clickable(top)).click()

    # Go back to list of years
    WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.LINK_TEXT, "View Past Seasons"))).click()
```

Use pickle to save list to prevent having to run Selenium again.

```{python}
#| eval: false
import pickle

with open('dayton_html.pkl', 'wb') as f:
    pickle.dump(pages, f, pickle.HIGHEST_PROTOCOL)
```


```{python}
# Reload list of HTML data if necessary
import pickle

with open('dayton_files/dayton_html.pkl', 'rb') as f:
    pages = pickle.load(f)
```

Use BeautifulSoup to parse HTML data and compile it into a data set.

```{python}
from bs4 import BeautifulSoup
import pandas as pd

# List of dataframes storing results from each page
dfs = []

for page in pages:
    doc = BeautifulSoup(page, "html.parser")

    event_name = doc.find(id="cs-org-scores-header").get_text()
    rank_col = doc.find_all("div", "rank")
    group_col = doc.find_all("div", "group")
    score_col = doc.find_all("div", "score")

    # Remove HTML tags
    rank_col = [x.get_text() for x in rank_col]
    group_col = [x.get_text() for x in group_col]
    score_col = [x.get_text() for x in score_col]

    # Create dataframe of this page of data
    data = {"Event": [event_name] * len(rank_col),
            "Rank": rank_col,
            "Group": group_col,
            "Score": score_col}
    event = pd.DataFrame(data)
    dfs.append(event)

# Final dataset
world_df = pd.concat(dfs, ignore_index = True)
```

Save the data set as a CSV file to avoid rerunning code.

```{python}
#| eval: false
world_df.to_csv('dayton_files/dayton_uncleaned.csv', index = False)
```

Read in CSV file.

```{python}
world_df = pd.read_csv("dayton_files/dayton_uncleaned.csv", index_col = False)
world_df.drop(world_df.columns[0], axis = 1, inplace = True)
```

# Data Analysis

Get basic info of the dataset.

```{python}
# First 5 rows of data
world_df.head()

# General info
world_df.info()
world_df.describe()
```

## Cleaning Data

Remove Color Guard and Winds events.

```{python}
# Convert types
world_df['Event'] = world_df['Event'].astype(str)
world_df['Rank'] = world_df['Rank'].astype(str)
world_df['Group'] = world_df['Group'].astype(str)
world_df['Score'] = pd.to_numeric(world_df['Score'])

# Remove Color Guard and Wind events
wrong_activity = world_df['Event'].str.contains("Winds") | world_df['Event'].str.contains("Color Guard") | world_df['Event'].str.contains("CG")
world_df = world_df[wrong_activity == False]
```

"2023 World Championships IO RD 1 & 3, SW, IW Prelims" is too much... I'm only going to look at Independent World and remove all other columns 

```{python}
# Independent World Groups
# piw = world_df['Event'].str.contains("IW") | world_df['Event'].str.contains("World Class") | world_df['Event'].str.contains("Independent World")
piw = world_df['Event'].str.contains("IW") | world_df['Event'].str.contains("World") | world_df['Event'].str.contains("Championships")

world_df = world_df[piw]
```

Add a column for the year

```{python}
world_df['Event'].unique()
```

```{python}
years = [x[0:4] for x in world_df['Event']] 
# 2014 is missing year in its event name
world_df.insert(1, "Year", years, True)
world_df.head()
```

Add a column for the day (Prelims, Semis, Finals)

```{python}
prelims = world_df['Event'].str.contains("Prelims")
semis = world_df['Event'].str.contains("Semis") | world_df['Event'].str.contains("Semifinals")
finals = world_df['Event'].str.contains("Finals")

world_df.insert(2, "Day", "", True)
world_df.loc[prelims, "Day"] = "Prelims"
world_df.loc[semis, "Day"] = "Semifinals"
world_df.loc[finals, "Day"] = "Finals"
```

```{python}
world_df.to_csv('dayton_files/dayton_cleaned.csv', index = False)
```

## Questions

Read in cleaned data set in R.
```{r}
dayton <- read.csv("dayton_files/dayton_cleaned.csv")
```

For any given group, is it likely to win on Finals day if they did not win on either Semifinals or Prelims day?

```{r}
# Remove all non-Percussion in 2016
# Remove all non "PS" / "PI" / "Concert" in 2023
# Correct year for 2014 (year missing in event name)
unique(dayton[["Event"]])
dayton <- dayton[grepl(pattern = "Percussion", dayton[["Event"]][dayton["Year"] == 2016]), ]
dayton[dayton["Year"] == 2016, ]
```



# Reflection

## Technical Learning

learning python syntax

`enumerate()`

debugging lots of errors

learning DOM and HTML 

**wgi is a dynamic website**

selenium quirks (can't click on an element if it's not visible on the page)

- basically dont do ANYTHING (finding an element, clikcing elemnt, scorlling to element) until sleenium recognize the leement is loaded

`StaleElementError`

different names for World Class finals through all the years.. 
    using IW and Championships will create some duplicates.. but its possible to remove duplicate data :D

Color Guard referred to with CG and Colorguard...

took like 15 mins to scrape the data LOL not even including 2020-2021 cuz those years did not exist

slightly different syntax of pandas data frames vs R
    i have an idea of the actions i want to do but no idea how to write the command to do it

LOADING TiMes... doing things efficiently
